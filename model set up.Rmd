---
title: "setting"
author: "Nick Borcherding"
date: "7/15/2021"
output: html_document
---

```{r}
library(keras)
tensorflow::tf$compat$v1$disable_eager_execution()
reference <- read.delim("~/Documents/GitHub/Trex/data/aa_props.tsv")
meta <- readRDS("~/Documents/GitHub/utility/data/processedData/meta.rds")
cdr3 <- unique(na.omit(meta$CTaa)) #remove NA and duplicates
cdr3 <- cdr3[!grepl("_NA|NA_", cdr3)] #remove without TRA or TRB
TRA <- stringr::str_split(cdr3, "_", simplify = T)[,1]
TRB <- stringr::str_split(cdr3, "_", simplify = T)[,2]
set.seed(123)


normalize <- function(x)
  {
      (x- min(x)) /(max(x)-min(x))
   }

AA.properties = "af"
col.ref <- grep(tolower(paste(AA.properties, collapse = "|")), colnames(reference))
column.ref <- unique(sort(col.ref))

chain <- list(TRA, TRB) 

names(chain) <- c("TRA", "TRB")
AA.properties <- c("AF", "KF", "both")
for (m in seq_along(chain)) {
    COI <- chain[[m]]
    for (n in seq_along(AA.properties)) {
      AA.ref <- AA.properties[n]
      array.reshape <- NULL
      for (j in seq_len(length(COI))) {
            tmp.CDR <- COI[j]
            refer <- unlist(strsplit(tmp.CDR, ""))
            refer <- c(refer, rep(NA, 50 - length(refer)))
            int <- reference[match(refer, reference$aa),]
            if (AA.ref == "both") {
                col.ref <- grep(tolower("AF|KF"), colnames(reference))
            } else {
                col.ref <- grep(tolower(paste(AA.ref, collapse = "|")), colnames(reference))
            }
            int <- int[,col.ref]
            array.reshape <- rbind(array.reshape,array_reshape(as.matrix(int), length(col.ref)*50))
      } 
      #Setting Up and Normalizing Data
      data <- array.reshape
      data[is.na(data)] <- 0
      
      for (z in seq_len(ncol(data))) {
        data[,z] <- normalize(data[,z])
      }
      data[is.na(data)] <- 0
      n.train <- nrow(data) * 0.8
      original_dim <- ncol(data)
      
        K <- keras::backend()
      
        
        batch_size <- 128
        original_dim <- ncol(data)
        latent_dim <- 30
        epsilon_std <- 1
        null.threshold<-0.05
      
      
        # data into folds
        n_test<-nrow(data)-n.train
        n_fold<-ceiling(nrow(data)/n_test)
        n_overlap<-round((n_fold*n_test-nrow(data))/(n_fold-1))
        cuts<-seq(1,nrow(data),n_test-n_overlap)
        cuts<-c(cuts[1:n_fold],nrow(data))
        all_output<-c()
      
        for(i in 1:n_fold){
        # Model definition --------------------------------------------------------
        x <- keras::layer_input(shape = c(original_dim))
        h1 <- keras::layer_dense(x, 128, activation = "relu")
        h2 <- keras::layer_dense(h1, 64, activation = "relu")
        z_mean <- keras::layer_dense(h2, latent_dim)
        z_log_var <- keras::layer_dense(h2, latent_dim)
      
        sampling <- function(arg){
          z_mean <- arg[, 1:(latent_dim)]
          z_log_var <- arg[, (latent_dim + 1):(2 * latent_dim)]
      
          epsilon <- keras::k_random_normal(
            shape = c(keras::k_shape(z_mean)[[1]]),
            mean=0.,
            stddev=epsilon_std
          )
      
          z_mean + keras::k_exp(z_log_var/2)*epsilon
        }
      
        # note that "output_shape" isn't necessary with the TensorFlow backend
        z <- keras::layer_concatenate(list(z_mean, z_log_var)) %>%
          keras::layer_lambda(sampling)
      
        # we instantiate these layers separately so as to reuse them later
        decoder_h <- keras::layer_dense(units = 64, activation = "relu")
        decoder_h2 <- keras::layer_dense(units = 128, activation = "relu")
        decoder_mean <- keras::layer_dense(units = original_dim, activation = "sigmoid")
        h_decoded <- decoder_h(z)
        h_decoded2 <-  decoder_h2(h_decoded)
        x_decoded_mean <- decoder_mean(h_decoded2)
      
        # end-to-end autoencoder
        vae <- keras::keras_model(x, x_decoded_mean)
      
        # encoder, from inputs to latent space
        encoder <- keras::keras_model(x, z_mean)
      
        # generator, from latent space to reconstructed inputs
        decoder_input <- keras::layer_input(shape = latent_dim)
        h_decoded <- decoder_h(decoder_input)
        h_decoded_2 <- decoder_h2(h_decoded)
        x_decoded_mean_2 <- decoder_mean(h_decoded_2)
        generator <- keras::keras_model(decoder_input, x_decoded_mean_2)
      
      
        vae_loss <- function(x, x_decoded_mean){
          xent_loss <- (original_dim/1.0)*keras::loss_binary_crossentropy(x, x_decoded_mean)
          kl_loss <- -0.5*keras::k_mean(1 + z_log_var - keras::k_square(z_mean) - keras::k_exp(z_log_var), axis = -1L)
          xent_loss + kl_loss
        }
      
        vae %>% keras::compile(optimizer = "rmsprop", loss = vae_loss)
      
      
        #start rotation-------
      
        V_range<-c()
      
      
      
          x_test <- data[cuts[i]:cuts[i+1]-1,]
          x_train <- data[setdiff(1:nrow(data),cuts[i]:cuts[i+1]-1),]
      
      
          # Model training ----------------------------------------------------------
      
          vae %>% keras::fit(
            x_train, x_train,
            shuffle = TRUE,
            epochs = epochs,
            batch_size = batch_size,
            validation_data = list(x_test, x_test)
          )
      
        
        }
        save_model_hdf5(encoder, paste0(names(chain)[m], "_", AA.properties[n], "_Encoder.h5"))
        rm(vae)
        rm(encoder)
    }
}


```

