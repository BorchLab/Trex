---
title: "Running from Trex"
author: "Nick Borcherding"
date: "7/6/2021"
output: html_document
---

```{r}
library(stringr)
library(stringdist)
library(amap)
library(dplyr)
library(muxViz)
library(igraph)
tmp <- readRDS("~/Documents/GitHub/Trex/Sample11_seuratObject.rds")
reference <- read.delim("~/Documents/GitHub/Trex/data/aa_props.tsv")
#igraph <- runTrex(tmp)

meta <- tmp[[]]
meta <- meta[!is.na(meta$cloneType),]
tmp <- subset(tmp, cells = rownames(meta))
```

```{r}
# Create a graph adjacency based on correlation distances between genes in  pairwise fashion.
g <- graph.adjacency(
  igraph[[1]],
  mode="undirected",
  weighted=TRUE,
  diag=FALSE
)
g <- simplify(g)
eigen <- spectrum(g, 
                 which = list(howmany = 30), 
                 algorithm = "arpack")

g_mst <- mst(g)
nodeTensor <- get.adjacency(g_mst)
nodeTensor[1:20,1:20]
plot(g_mst)

make.knn.graph<-function(D,k){
  # calculate euclidean distances between cells
  dist<-as.matrix(dist(D))
  # make a list of edges to k nearest neighbors for each cell
  edges <- mat.or.vec(0,2)
  for (i in 1:nrow(dist)){
    # find closes neighbours
    matches <- setdiff(order(dist[i,],decreasing = F)[1:(k+1)],i)
    # add edges in both directions
    edges <- rbind(edges,cbind(rep(i,k),matches))  
    edges <- rbind(edges,cbind(matches,rep(i,k)))  
  }
  # create a graph from the edgelist
  graph <- graph_from_edgelist(edges,directed=F)
  V(graph)$frame.color <- NA
  # make a layout for visualizing in 2D
  set.seed(1)
  g.layout<-layout_with_fr(graph)
  return(list(graph=graph,layout=g.layout))        
}

library(FNN)
x <- get.knn(multi.network[[1]], k = 30)
```

#Notes:

As of right now the following works to produce PCA that can be added to the seurat object and perform WNN on

Issues: 
1. Eigen vectors after 1d are not helpful
 - So this is not the case the first eigenvector actually is representative of the large issue of NA values vs non-NA-value, eliminating the first eigen dimension recovers the better seperation of clones
2. The WNN actually adds T cells to non-T cell and non T cells form two majory clusters that are a mix of T/B/and myeloid

Things I have Tried:
- Change the ARPACK to call largest algebraic value (not helpful)
- Change the assay call in DimReduction creation of Trex
- What happens with lower thresholds vs higher thresholds? - Changes the vector not the eigen values
-Remove non-T cells? Still mixing of CD4 and CD8 T cells (this might change wehn adding additional metrixs)

Guess
- Change nature of multiplexed list
    - What if threhsold is emilimated and graphs are made by raw distance calc?
    - Any change if we remove the diagnols?

    
```{r}
N <- simplify(igraph)
eigen <- spectrum(N, 
                 which = list(howmany = 30), 
                 algorithm = "arpack")
plot(eigen$vectors[,4], eigen$vectors[,2])


#SCG <- scg(N, ev = 1, nt = 2)
#ProjectionMatrix <- magic....(not done yet)

ProjectionMatrix <- eigen$vectors[,1:30]
rownames(ProjectionMatrix) <- rownames(tmp[[]])

unwanted_genes <- "^IGHV*|^IGHJ*|^IGHD*|^IGKV*|^IGLV*|^TRBV*|^TRBD*|^TRBJ*|^TRDV*|^TRDD*|^TRDJ*|^TRAV*|^TRAJ*|^TRGV*|^TRGJ*"
remove_genes <- tmp[["RNA"]]@var.features %in% unwanted_genes
tmp[["RNA"]]@var.features = tmp[["RNA"]]@var.features[!remove_genes]

#Making essientially a pca out of the sparse matrix
Trex <- CreateDimReducObject(
    embeddings = ProjectionMatrix,
    loadings = ProjectionMatrix,
    projected = ProjectionMatrix,
    stdev = rep(0, ncol(ProjectionMatrix)),
    key = "Trex",
    jackstraw = NULL,
    misc = list()
  )
tmp@reductions[["Trex"]] <- Trex

tmp <- FindNeighbors(tmp)
tmp <- FindNeighbors(tmp, reduction = "Trex")
wnn  <- FindMultiModalNeighbors(
  tmp, 
  reduction.list = list("pca", "Trex"), 
  dims.list = list(1:30, 1:20), 
  modality.weight.name = "RNA.weight"
)

tmp <- RunUMAP(wnn, nn.name = "weighted.nn", reduction.name = "wnn.umap", reduction.key = "wnnUMAP_")
tmp <- RunUMAP(tmp, reduction = 'pca', dims = 1:30, assay = 'RNA', 
              reduction.name = 'rna.umap', reduction.key = 'rnaUMAP_')
tmp <- RunUMAP(tmp, reduction = 'Trex', dims = 1:20, 
              reduction.name = 'trex.umap', reduction.key = 'trexUMAP_')
tmp <- FindClusters(tmp, graph.name = "wsnn", algorithm = 3, verbose = FALSE)

DimPlot(tmp)
FeaturePlot(tmp, features = "CD4")
DimPlot(tmp, group.by = "cloneType")

DimPlot(tmp, reduction = 'rna.umap')
FeaturePlot(tmp, reduction = 'rna.umap', features = "CD3G")
FeaturePlot(tmp, reduction = 'rna.umap', features = "CD4")
FeaturePlot(tmp, reduction = 'rna.umap', features = "CD8A")
DimPlot(tmp, reduction = 'trex.umap')
FeaturePlot(tmp, reduction = 'trex.umap', features = "CD3G")
FeaturePlot(tmp, reduction = 'trex.umap', features = "CD4")
FeaturePlot(tmp, reduction = 'trex.umap', features = "CD8A")
DimPlot(tmp, reduction = 'wnn.umap')
FeaturePlot(tmp, reduction = 'wnn.umap', features = "CD3G")
FeaturePlot(tmp, reduction = 'wnn.umap', features = "CD4")
FeaturePlot(tmp, reduction = 'wnn.umap', features = "CD8A")
FeaturePlot(tmp, reduction = 'wnn.umap', features = "FOXP3")
DimPlot(tmp, reduction = 'wnn.umap', group.by = "cloneType")
DimPlot(tmp, reduction = 'wnn.umap')
```
```{r}
h2o.init()
x <- int
#y <- matrix(ncol = (80-nrow(x)), nrow = 28)
y <- matrix(nrow = (80-nrow(x)), ncol = 28)
colnames(y) <- colnames(x[,2:29])
#x <- cbind(t(x[,2:29]), y)
x <- rbind(x[,2:29], y)
h2o.init()
z <- as.h2o(x)
ae1 <- h2o.deeplearning(
  x = seq_along(z), 
  training_frame = z,
  autoencoder = TRUE,
  hidden = c(100,50,30, 1, 30 ,50,100), 
  activation = "Tanh",
  seed = 123
)

ae1_coding <- h2o.deepfeatures(ae1, z, layer = 1)
ae2_coding <- h2o.deepfeatures(w1, w1, layer = 1)

ae2 <- h2o.deeplearning(
  x = ae1_coding, 
  training_frame = ae1_coding,
  autoencoder = TRUE,
  hidden = 2, 
  activation = "Tanh",
)

layers <- c(100,50,20)
  args <- list(activation="Tanh", epochs=1, l1=1e-5)
  ae <- get_stacked_ae_array(z, layers, args)
  
w1 <- as.matrix(h2o.deepfeatures(ae1, z, layer=4))
names(w1) <- gsub("DF", paste0("L",2,sep=""), names(z)) 
w2 = h2o.deepfeatures(ae[[2]], w1, layer = 1)
rownames(w1) <- int$aa
plot(w1)

get_stacked_ae_array <- function(training_data,layers,args){  
    vector <- c()
    index = 0
    for(i in 1:length(layers)){    
      index = index + 1
      ae_model <- do.call(h2o.deeplearning, 
                          modifyList(list(x=names(training_data),
                                          training_frame=training_data,
                                          autoencoder=TRUE,
                                          hidden=layers[i]),
                                     args))
      training_data = h2o.deepfeatures(ae_model,training_data,layer=1)
      
      names(training_data) <- gsub("DF", paste0("L",index,sep=""), names(training_data)) 
      vector <- c(vector, ae_model)    
    }
    vector
  }
```

```{r}
check.deeplearning_stacked_autoencoder <- function() {  
  # this function builds a vector of autoencoder models, one per layer
  get_stacked_ae_array <- function(training_data,layers,args){  
    vector <- c()
    index = 0
    for(i in 1:length(layers)){    
      index = index + 1
      ae_model <- do.call(h2o.deeplearning, 
                          modifyList(list(x=names(training_data),
                                          training_frame=training_data,
                                          autoencoder=TRUE,
                                          hidden=layers[i]),
                                     args))
      training_data = h2o.deepfeatures(ae_model,training_data,layer=1)
      
      names(training_data) <- gsub("DF", paste0("L",index,sep=""), names(training_data)) 
      vector <- c(vector, ae_model)    
    }
    vector
  }
  
  # this function returns final encoded contents
  apply_stacked_ae_array <- function(data,ae){
    index = 0
    for(i in 1:length(ae)){
      index = index + 1
      data = h2o.deepfeatures(ae[[i]],data,layer=1)
      names(data) <- gsub("DF", paste0("L",index,sep=""), names(data)) 
    }
    data
  }
  
  TRAIN <- "bigdata/laptop/mnist/train.csv.gz"
  TEST <- "bigdata/laptop/mnist/test.csv.gz"
  response <- 785
  
  # set to T for RUnit
  # set to F for stand-alone demo
  if (TRUE) {
    train_hex <- h2o.importFile(locate(TRAIN))
    test_hex  <- h2o.importFile(locate(TEST ))
  } else {
    library(h2o)
    h2o.init(nthreads=-1)
    homedir <- paste0(path.expand("~"),"/h2o-dev/") #modify if needed
    train_hex <- h2o.importFile(path = paste0(homedir,TRAIN), header = FALSE, sep = ',')
    test_hex  <- h2o.importFile(path = paste0(homedir,TEST), header = FALSE, sep = ',')
  }
  train <- train_hex[,-response]
  test  <- test_hex [,-response]
  train_hex[,response] <- as.factor(train_hex[,response])
  test_hex [,response] <- as.factor(test_hex [,response])
  
  ## Build reference model on full dataset and evaluate it on the test set
  model_ref <- h2o.deeplearning(training_frame=train_hex, x=1:(ncol(train_hex)-1), y=response, hidden=c(10), epochs=1)
  p_ref <- h2o.performance(model_ref, test_hex)
  h2o.logloss(p_ref)
  
  ## Now build a stacked autoencoder model with three stacked layer AE models
  ## First AE model will compress the 717 non-const predictors into 200
  ## Second AE model will compress 200 into 100
  ## Third AE model will compress 100 into 50
  layers <- c(200,100,50)
  args <- list(activation="Tanh", epochs=1, l1=1e-5)
  ae <- get_stacked_ae_array(train, layers, args)
  
  ## Now compress the training/testing data with this 3-stage set of AE models
  train_compressed <- apply_stacked_ae_array(train, ae)
  test_compressed <- apply_stacked_ae_array(test, ae)
  
  ## Build a simple model using these new features (compressed training data) and evaluate it on the compressed test set.
  train_w_resp <- h2o.cbind(train_compressed, train_hex[,response])
  test_w_resp <- h2o.cbind(test_compressed, test_hex[,response])
  model_on_compressed_data <- h2o.deeplearning(training_frame=train_w_resp, x=1:(ncol(train_w_resp)-1), y=ncol(train_w_resp), hidden=c(10), epochs=1)
  p <- h2o.performance(model_on_compressed_data, test_w_resp)
  h2o.logloss(p)
  
  
}


hyper_grid <- list(hidden = list(
  c(50), 
  c(100), 
  c(300, 100, 300), 
  c(50,25, 50), 
  c(100, 50, 100), 
  c(250, 100, 50, 100, 250)))

ae_grid <- h2o.grid(
  algorithm = "deeplearning",
  x = seq_along(z), 
  training_frame = z,
  autoencoder = TRUE,
  grid_id = "autoencoder_grid", 
  hyper_params = hyper_grid,
  activation = "Tanh",
  seed = 123
  
)

ae_grid@summary_table

h2o.getGrid("autoencoder_grid", sort_by = "mse")
```

